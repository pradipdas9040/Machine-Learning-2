{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "BERT (Bidirectional Encoder Representation from Transformers) is a state of the art model desined for natural language understanding. It's based on the Transformer architecture and is notable for its bidirectional training approch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT's Core Concept\n",
    "\n",
    "BERT improves on previous models like GPT (which is unidirectional, i.e., left-to-right). BERT is **bidirectional**, meaning it looks at both the left and right context in all layers of the encoder, which leads to better understanding of the meaning of a word in context.\n",
    "\n",
    "- **Main Concept**\n",
    "    - **BIdirectional:** Unlike traditional language model that read the text in a single direction (left-to-right or right-to-left), BERT reads the text in both directions simultaneously. This allows the model to understand the context of a word based on the word that come before and after it.\n",
    "\n",
    "    - **Masked language Model:** During pre-training, BERT randomly masked some tokens in a sentence and then tries to predict those masked tokens. This forces the model to build a deep understanding of both sides of the sentence (left and right context).\n",
    "    \n",
    "    - **Next Sentence Predction:** To help BERT understand relationships between sentence, it was trained to predict whether one sentence follows another. This is crucial for tasks like question answering and language inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Architecture\n",
    "**1. Input Representation:**\n",
    "\n",
    "BERT processes text in the form of tokens. Before feeding input to BERT, text is tokenized into subword units using WordPiece tokenizer.\n",
    "\n",
    "Each input tokeb is represented by the sum of three embeddings:\n",
    "- Token Embedding: A vector for each token in the input sequense.\n",
    "\n",
    "- Segment Embedding: BERT can takes pair of sentence as input. Segment embeddings help distinguish between the two sentences.\n",
    "\n",
    "- Positional Embeddings: Adds positional information to each token so that the model can understand the order of words in the sequence.\n",
    "\n",
    "**Example:** For the input: *\"The cat sat on the mat.\"*, BERT adds special tokens:\n",
    "\n",
    "- `[CLS] The cat sat on the mat. [SEP]` Here, `[CLS]` is a special token added at the beginning (used for classification task) and `[SEP]` marks the end of a sentence.\n",
    "\n",
    "**2. Encoder Stack:**\n",
    "\n",
    "BERT uses a multi-layered transformer encoder. The standerd BERT model has 12 layers of transformers (BERT-Base) or 24 layer (BERT-Large). Each layer contains:\n",
    "\n",
    "- **Multi-Head Self-Attention:** This layer allows the model to attend to different words in the sequence simultaneously, capturing word relationships and dependencies. Each head capture different contextual information.\n",
    "\n",
    "- **Feed-Forward Neural Network:** After attention, the output goes through a fully connected feed forward network to refine the learned representation.\n",
    "\n",
    "- **Layer Normalization & Residual Connections:** Normalization helps stabilize training and residual connection anable the model to pass information across layers efficiently.\n",
    "\n",
    "___\n",
    "**What does Layer Normalization do?**\n",
    "\n",
    "Layer Normalization is a technique used in machine learning and artificial intelligence to normalize the inputs of a neural network layer. It ensures that the inputs have a consistent distribution and reduces the internal covariate shift problem that can occur during training. By normalizing the inputs, Layer Normalization enhances the stability and generalization of the network.\n",
    "\n",
    "Layer Normalization directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. It works well for RNNs and improves both the training time and the generalization performance of several existing RNN models. More recently, it has been used with Transformer models.\n",
    "\n",
    "We compute the layer normalization statistics over all the hidden units in the same layer as follows:\n",
    "\n",
    "$$\n",
    "\\mu^l = \\frac{1}{H}\\sum_{i=1}^{H} a_i^l\\\\\n",
    "\\sigma^l = \\sqrt{\\frac{1}{H}\\sum_{i=1}^{H} (a_i^l-\\mu^l)^2}\n",
    "$$\n",
    "\n",
    "where $H$ denotes the number of hidden units in a layer. Under layer normalization, all the hidden units in a layer share the same normalization terms $\\mu$ and $\\sigma$, but different training cases have different normalization terms. Unlike batch normalization, layer normalization does not impose any constraint on the size of the mini-batch and it can be used in the pure online regime with batch size $1$.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training BERT\n",
    "\n",
    "When training language models, there is a challenge of defining a predction goal. Many models predict the next word in a sentence (e.g., \"The child came home from ___\"), a directional approch which inherently limit contect learning. To overcome this challenge, BERT uses two training straregies:\n",
    "\n",
    "**Masked LM (MLM):**\n",
    "\n",
    "Before feeding word sequences into BERT, 15% of the word in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked words in the sequence. In technical terms, the predction of the output words requires:\n",
    "\n",
    "1. Adding a classification layer on top of the encoder output.\n",
    "\n",
    "2. Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.\n",
    "\n",
    "3. Calculating the probability of each word in the vocabulary with softmax.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/0*ViwaI3Vvbnd-CJSQ.png\" width=\"450\">\n",
    "\n",
    "The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words. As a consequence, the model converges slower than directional models, a characteristic which is offset by its increased context awareness.\n",
    "\n",
    "**Next Sentence Prediction (NSP):**\n",
    "\n",
    "In the BERT training process, the model receives pairs of sequences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The asumption is that the random sentence will be disconnected from the first sentence.\n",
    "\n",
    "To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:\n",
    "\n",
    "1. A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n",
    "\n",
    "2. A sentence (or segment) embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.\n",
    "\n",
    "3. A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper.\n",
    "\n",
    "**Example input:**\n",
    "\n",
    "- Sentence A: \"The cat sat on the mat.\"\n",
    "- Sentence B: \"It started to purr.\"\n",
    "\n",
    "The input becomes:\n",
    "- [CLS] The cat sat on the mat [SEP] It started to purr [SEP]\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/0*m_kXt3uqZH9e7H4w.png\" width=\"450\">\n",
    "\n",
    "To predict if the second sentence is indeed connected to the first, the following steps are performed:\n",
    "\n",
    "1. The entire input sequence goes through the Transformer encoder model.\n",
    "\n",
    "    - key Output:\n",
    "\n",
    "        - [CLS] Token Representation:\n",
    "            \n",
    "            - The output embedding for the [CLS] token is particularly important in NSP. It acts as a summary of the entire input sequence (i.e., the two sentences combined).\n",
    "            - The contextualized representation of [CLS] is used for the NSP task, and it captures information from both Sentence A and Sentence B.\n",
    "            \n",
    "        - Contextualized Token Embeddings:\n",
    "            - Each token in the sequence (e.g., \"The\", \"cat\", \"sat\", etc.) has its own output embedding, which is a vector that captures the token's meaning in the context of the sentence pair. These embeddings are not directly used for NSP but are available for other tasks.\n",
    "\n",
    "2. The output of the [CLS] token is transformed into a 2Ã—1 shaped vector, using a simple classification layer (learned matrices of weights and biases).\n",
    "\n",
    "3. Calculating the probability of IsNextSequence with softmax.\n",
    "\n",
    "When training the BERT model, Masked LM and Next Sentence Prediction are trained together, with the goal of minimizing the combined loss function of the two strategies.\n",
    "\n",
    "- **Summary of Outputs in NSP:**\n",
    "    1. Contextualized Token Embeddings for each token in the input sequence, capturing token meaning in context.\n",
    "\n",
    "    2. [CLS] Token Embedding: A summary of the entire input (Sentence A + Sentence B) used for the NSP task.\n",
    "\n",
    "    3. NSP Classification Output: The output of a classification layer that predicts whether Sentence B logically follows Sentence A (binary classification).\n",
    "    \n",
    "    Thus, the NSP task leverages the output of the [CLS] token from the Transformer encoder, using it to determine sentence-level relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning BERT\n",
    "\n",
    "Once BERT is pre-trained, it can be fine-tuned for specific downstream task like text classification, named entity recognition or question answering. This is done by adding task-specific layers on top of BERT's pre-trained layers and training the model on task-specific labeled data.\n",
    "\n",
    "1. GLUE Benchmark Tasks (General Languuage Understanding Evaluation)\n",
    "    \n",
    "    GLUE is a widely used benchmark for evaluating models on multiple NLP task. BERT was fine-tuned on the following tasks from GELU.\n",
    "\n",
    "    1. CoLA (Corpus of Linguistic Acceptability)\n",
    "        \n",
    "        - Task: Sentence Classification, determining whether a sentence is grammatically acceptable or not.\n",
    "\n",
    "        - Metric: Mmatthews Correlation Cofficient (MCC)\n",
    "\n",
    "        - BERT Result: $60.5$ (for BERT-Large), which outperforms the previous state-of-art model by significant margin.\n",
    "    \n",
    "    2. SST-2 (Stanford Sentiment Treebank)\n",
    "\n",
    "        - Task: Binary sentiment classification (positive or negitive sentiment) on movie reviews.\n",
    "\n",
    "        - Metric: Accuracy\n",
    "\n",
    "        - BERT Result: $94.9\\%$ (BERT-Large), achiving near-human performance.\n",
    "\n",
    "    3. MRPC (Microsoft Research Paraphrase Corpus)\n",
    "\n",
    "        - Task: Classify whether two sentence are paraphrases of each other.\n",
    "\n",
    "        - Metric: F!-score/Accuracy\n",
    "\n",
    "        - BERT Result: $89.3$ (F1-score), $86.7\\%$ (Accuracy) on BERT-Large.\n",
    "\n",
    "    4. STS-B (Semantic Textual Similarity Benchmark)\n",
    "\n",
    "        - Task: Predict the similarity between two sentences (on a scale of 1 to 5).\n",
    "\n",
    "        - Metric: Pearson/sperman Correlation\n",
    "\n",
    "        - BERT Result: $90.0$ (BERT-Large), indicating strong sentence-pair similarity prediction.\n",
    "\n",
    "    5. QQP (Quora Question Pairs)\n",
    "\n",
    "        - Task: Determine if two question from Quora are paraphrases.\n",
    "\n",
    "        - Metric: F1-score/Accuracy\n",
    "\n",
    "        - BERT Result: $88.5$ (F1-score), $91.3\\%$ (Accuracy) on BERT-Large.\n",
    "\n",
    "    6. MNLI (Multi-Genre Natural Language Inference)\n",
    "\n",
    "        - Task: Sentence-pair classification task, where the goal is to predict whether the second sentence is an entailment, contradiction, or neutral to the first sentence.\n",
    "        \n",
    "        - Metric: Accuracy\n",
    "        \n",
    "        - BERT Result: $86.6\\%$ (matched), $85.9\\%$ (mismatched) with BERT-Large.\n",
    "\n",
    "    7. QNLI (Question Natural Language Inference)\n",
    "\n",
    "        - Task: Sentence-pair classification derived from the Stanford Question Answering Dataset (SQuAD), where the goal is to determine whether a given sentence contains the answer to a question.\n",
    "\n",
    "        - Metric: Accuracy\n",
    "\n",
    "        - BERT Result: $92.7\\%$ (BERT-Large).\n",
    "\n",
    "    8. RTE (Recognizing Textual Entailment)\n",
    "\n",
    "        - Task: Predict whether one sentence entails another.\n",
    "\n",
    "        - Metric: Accuracy\n",
    "\n",
    "        - BERT Result: $70.1\\%$ (BERT-Large).\n",
    "\n",
    "    9. WNLI (Winograd NLI)\n",
    "\n",
    "        - Task: Resolve pronoun reference ambiguities to determine sentence entailment.\n",
    "\n",
    "        - Metric: Accuracy\n",
    "\n",
    "        - BERT Result: $65.1\\%$ (BERT-Large). The WNLI task is quite difficult and models often struggle with it.\n",
    "\n",
    "2. SQuAD (Stanford Question Answering Dataset)\n",
    "\n",
    "    SQuAD is a popular dataset for question-answering tasks, where the model must predict the span of text that answers a given question from a paragraph.\n",
    "\n",
    "    1. SQuAD V1.1\n",
    "        \n",
    "        - Task: Given a passage, the model must predict the exact span of words that answers the question.\n",
    "\n",
    "        - Metrics: Exact Match (EM) and F1-score\n",
    "\n",
    "        - BERT Result: $84.1$ (EM), $90.9$ (F1-score) for BERT-Large\n",
    "\n",
    "    2. SQuAD V2.0\n",
    "\n",
    "        - Task: In addition to answering questions, the model must also handle questions that have no answer in the passage.\n",
    "\n",
    "        - Metrics: Exact Match (EM) and F1-score\n",
    "\n",
    "        - BERT Result: $78.7$ (EM), $81.9$ (F1-score) for BERT-Large.\n",
    "\n",
    "3. NER (Named Entity Recognition) on CoNLL-2003\n",
    "\n",
    "    The task is to identify named entities like people, organizations, locations, etc., in text.\n",
    "\n",
    "    - Task: Token-level classification where each token is labeled as belonging to an entity (e.g., PERSON, LOCATION).\n",
    "\n",
    "    - Metric: F1-score\n",
    "\n",
    "    - BERT Result: $92.8\\%$ F1-score (BERT-Large), outperforming previous state-of-the-art models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of BERT\n",
    "\n",
    "BERT is used in a variety of NLP tasks:\n",
    "\n",
    "1. Text Classification: Predicting a class label (e.g., spam detection).\n",
    "\n",
    "2. Named Entity Recognition (NER): Identifying entities in a text like person names or locations.\n",
    "\n",
    "3. Question Answering: BERT has been used for tasks like SQuAD (Stanford Question Answering Dataset).\n",
    "\n",
    "4. Natural Language Inference: Determining if one sentence logically follows from another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Variants\n",
    "\n",
    "- BERT-Base: 12 layers (transformer blocks), 12 attention heads, 110M parameters.\n",
    "\n",
    "- BERT-Large: 24 layers, 16 attention heads, 340M parameters.\n",
    "\n",
    "- DistilBERT: A lighter version of BERT, optimized for speed and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ref\n",
    "\n",
    "- https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "\n",
    "- https://arxiv.org/pdf/1810.04805\n",
    "\n",
    "- https://paperswithcode.com/method/batch-normalization\n",
    "\n",
    "- https://paperswithcode.com/method/layer-normalization\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-bert/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
