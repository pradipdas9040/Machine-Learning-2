{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ 1. What is XGBoost?\n",
    "XGBoost (Extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. It is designed to be highly efficient, scalable, and accurate. It builds an ensemble of decision trees sequentially, where each new tree tries to correct the errors made by the previous ones using gradient descent.\n",
    "\n",
    "It supports:\n",
    "\n",
    "- Regression\n",
    "- Classification\n",
    "- Ranking\n",
    "- Custom objective functions\n",
    "\n",
    "#### ‚öôÔ∏è Scenario: Custom Objective Function in XGBoost\n",
    "Let‚Äôs say you're doing regression, but you want to use Mean Absolute Error (MAE) instead of the default Mean Squared Error (MSE). XGBoost does not support MAE directly because it's not differentiable everywhere ‚Äî but you can still plug it in as a custom objective using smoothed approximation.\n",
    "\n",
    "``` python\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 1. Create synthetic regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 2. DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# 3. Define custom MAE (smoothed with epsilon)\n",
    "def custom_mae_objective(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    grad = np.sign(preds - labels)  # first derivative\n",
    "    hess = np.ones_like(preds)      # second derivative (approx.)\n",
    "    return grad, hess\n",
    "\n",
    "# 4. Optional: custom eval metric\n",
    "def mae_eval_metric(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(labels, preds)\n",
    "\n",
    "# 5. Train with custom objective\n",
    "params = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.1,\n",
    "    'silent': 1,\n",
    "}\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=100,\n",
    "    obj=custom_mae_objective,\n",
    "    feval=mae_eval_metric,\n",
    "    evals=[(dtest, 'test')],\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# 6. Predict\n",
    "preds = bst.predict(dtest)\n",
    "print(\"Final MAE:\", mean_absolute_error(y_test, preds))\n",
    "```\n",
    "#### üß† Notes:\n",
    "- `obj=custom_mae_objective`: tells XGBoost to use our custom gradient & hessian.\n",
    "- `feval=mae_eval_metric`: custom evaluation printed during training.\n",
    "- The MAE gradient is sign(pred - label) and hessian is approximated as constant for simplicity.\n",
    "\n",
    "## ‚úÖ 2. How does XGBoost differ from traditional gradient boosting?\n",
    "XGBoost improves traditional gradient boosting in several key ways:\n",
    "\n",
    "Feature| Traditional GBM|\tXGBoost\n",
    "|---|---|---|\n",
    "Regularization\t|No or minimal|\tL1 (alpha) and L2 (lambda)\n",
    "Parallelization\t|No\t|Yes (during tree construction)\n",
    "Tree Pruning\t|Pre-pruning\t|Post-pruning (using gamma)\n",
    "Handling Missing Values|\tManual or preprocessed\t|Built-in automatic handling\n",
    "Efficiency\t|Slower\t|Fast (due to optimized C++ backend)\n",
    "Second-order optimization|\tOptional|\tAlways uses gradient + hessian (2nd order)\n",
    "\n",
    "In short, XGBoost is faster, more regularized, and more scalable than traditional GBM.\n",
    "\n",
    "## ‚úÖ 3. What kind of problems can XGBoost solve?\n",
    "XGBoost can handle a wide variety of machine learning tasks:\n",
    "\n",
    "- Regression (e.g., predicting house prices)\n",
    "- Binary classification (e.g., spam detection)\n",
    "- Multiclass classification (e.g., digit recognition)\n",
    "- Ranking (e.g., search engine ranking)\n",
    "- Time series prediction (with feature engineering)\n",
    "- Anomaly detection\n",
    "\n",
    "It also supports custom loss functions, making it highly flexible.\n",
    "\n",
    "## ‚úÖ 4. What are the main advantages of using XGBoost?\n",
    "Here are the key advantages:\n",
    "\n",
    "- üîÑ Regularization (L1 & L2) ‚Üí prevents overfitting\n",
    "- ‚ö° High performance ‚Üí fast training with parallel processing\n",
    "- ü§ñ Automatic handling of missing values\n",
    "- üì¶ Built-in cross-validation and early stopping\n",
    "- üß† Handles large datasets and sparse data efficiently\n",
    "- üìä Feature importance extraction for interpretability\n",
    "- üß± Custom objective functions support\n",
    "\n",
    "## ‚úÖ 5. What are the key parameters in XGBoost?\n",
    "Here are some core hyperparameters grouped by function:\n",
    "\n",
    "#### üîß Tree Structure:\n",
    "- `max_depth`: Maximum depth of a tree\n",
    "- `min_child_weight`: Minimum sum of instance weight in a child\n",
    "- `gamma`: Minimum loss reduction to make a split\n",
    "- `subsample`: Fraction of training data for each tree\n",
    "- `colsample_bytree`: Fraction of features used per tree\n",
    "\n",
    "#### üéØ Learning:\n",
    "- `eta` (or learning_rate): Step size shrinkage\n",
    "- `n_estimators`: Number of trees (boosting rounds)\n",
    "- `objective`: Loss function (e.g., \"reg:squarederror\", \"binary:logistic\")\n",
    "\n",
    "#### üìè Regularization:\n",
    "- `lambda`: L2 regularization term\n",
    "- `alpha`: L1 regularization term\n",
    "\n",
    "#### ‚öôÔ∏è Others:\n",
    "- `booster`: Booster type (\"gbtree\", \"gblinear\", \"dart\")\n",
    "- `scale_pos_weight`: Balances classes in imbalanced datasets\n",
    "- `early_stopping_rounds`: For automatic stopping on validation performance\n",
    "\n",
    "## ‚úÖ 6. How does XGBoost handle missing values?\n",
    "XGBoost handles missing values automatically during training. It learns the best direction (left or right) to take when a feature is missing at a split by evaluating which path improves performance the most.\n",
    "\n",
    "- No need for imputation.\n",
    "- During prediction, if a feature is missing, XGBoost uses the learned default direction.\n",
    "\n",
    "## ‚úÖ 7. What is the difference between eta and learning_rate in XGBoost?\n",
    "They are aliases ‚Äî both refer to the same parameter:\n",
    "\n",
    "- Controls how much each new tree contributes to the final prediction.\n",
    "- Lower values mean slower learning but better generalization.\n",
    "- Often used with n_estimators: lower eta, higher n_estimators.\n",
    "\n",
    "## ‚úÖ 8. What is the role of max_depth and min_child_weight?\n",
    "These control tree complexity and help prevent overfitting:\n",
    "\n",
    "Parameter|\tDescription\t|Effect\n",
    "|---|---|---|\n",
    "max_depth\t|Max depth of trees\t|Higher ‚Üí more complex model\n",
    "min_child_weight\t|Minimum sum of weights in a child node\t|Higher ‚Üí more conservative splits\n",
    "\n",
    "- A lower max_depth generalizes better on simpler patterns.\n",
    "- A higher min_child_weight avoids splits on noise or small samples.\n",
    "\n",
    "## ‚úÖ 9. How does XGBoost perform regularization?\n",
    "XGBoost adds L1 and L2 regularization to its objective function:\n",
    "\n",
    "- lambda ‚Üí L2 regularization on leaf weights (Ridge)\n",
    "- alpha ‚Üí L1 regularization on leaf weights (Lasso)\n",
    "- gamma ‚Üí penalty for adding a new split (controls tree complexity)\n",
    "\n",
    "üìå Regularization term:\n",
    "$$\n",
    "\\Omega(f) = \\gamma T+\\frac{1}{2}\\lambda \\sum w_j^2 + \\alpha \\sum |w_j|\n",
    "$$\n",
    "\n",
    "## ‚úÖ 10. What is the difference between GBT and XGBoost in terms of optimization?\n",
    "Aspect|\tTraditional GBT|\tXGBoost\n",
    "|---|---|---|\n",
    "Optimization\t|First-order (uses gradients)\t|Second-order (uses gradient & hessian)\n",
    "Regularization\t|Minimal or none\t|Explicit L1, L2, and split regularization\n",
    "Parallelization\t|Limited\t|Efficient parallelized tree construction\n",
    "Tree Pruning\t|Pre-pruning\t|Post-pruning (prunes after building)\n",
    "\n",
    "XGBoost is faster, more regularized, and more accurate due to second-order optimization and advanced regularization.\n",
    "\n",
    "## ‚úÖ 11. How does XGBoost use second-order derivatives in training?\n",
    "XGBoost uses a second-order Taylor approximation of the loss function:\n",
    "\n",
    "$$\n",
    "Loss \\approx \\sum[g_i w+\\frac{1}{2}h_iw^2]\n",
    "$$\n",
    "\n",
    "- $g_i$: first-order gradient $(\\partial Loss/\\partial pred)$\n",
    "- $h_i$: second-order gradient or hessian $(\\partial^2 Loss/\\partial pred^2)$\n",
    "- This improves convergence and enables better split decisions.\n",
    "\n",
    "Each candidate split is evaluated based on gain, using both gradients and hessians.\n",
    "\n",
    "## ‚úÖ 12. How does XGBoost calculate feature importance?\n",
    "XGBoost provides three built-in ways to compute feature importance:\n",
    "\n",
    "Type|\tDescription\n",
    "|---|---|\n",
    "weight  |Number of times a feature is used in splits\n",
    "gain\t|Average gain when feature is used in splits\n",
    "cover\t|Number of samples affected by the splits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
