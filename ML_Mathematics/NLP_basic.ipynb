{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Overview\n",
    "Text classification is the process of assigning predefined categories to text data based on its content. It is a fundamental task in natural language processing (NLP) with applications in spam detection, sentiment analysis, topic labeling, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps in Text Classification\n",
    "\n",
    "- **Data Collection:** Gather labeled data for training and evaluation.\n",
    "- **Text Preprocessing:**\n",
    "    - Tokenization: Splitting text into words or subwords.\n",
    "    - Lowercasing: Converting all text to lowercase to reduce variability.\n",
    "    - Stopword Removal: Removing common words like \"and,\" \"the,\" which may not carry significant meaning.\n",
    "    - Stemming/Lemmatization: Reducing words to their root form (e.g., \"running\" ‚Üí \"run\").\n",
    "    - Handling Special Characters: Removing or encoding punctuation and emojis.\n",
    "- **Feature Extraction:**\n",
    "    - Bag of Words (BoW): Represents text as a collection of word frequencies.\n",
    "    - TF-IDF: Assigns weights to terms based on their importance.\n",
    "    - Word Embeddings: Converts text into dense vectors using models like Word2Vec, GloVe, or FastText.\n",
    "    - Sentence Embeddings: Captures the meaning of entire sentences using models like BERT or Sentence Transformers.\n",
    "- **Model Training:** Train a classification model using the extracted features.\n",
    "- **Evaluation:** Use metrics like accuracy, F1-score, and ROC-AUC to assess performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Models Used in Text Classification\n",
    "\n",
    "**Traditional Machine Learning Models**\n",
    "\n",
    "These models are used with features like BoW, TF-IDF, or manually engineered features.\n",
    "\n",
    "- **Naive Bayes:**\n",
    "    - Assumes features are independent.\n",
    "    - Simple and effective for tasks like spam detection.\n",
    "\n",
    "- **Logistic Regression:**\n",
    "    - Predicts probabilities for each category.\n",
    "    - Works well with large and sparse feature spaces.\n",
    "\n",
    "- **Support Vector Machines (SVMs):**\n",
    "    - Finds the hyperplane that best separates classes.\n",
    "    - Effective for high-dimensional text data.\n",
    "\n",
    "- **Decision Trees/Random Forests:**\n",
    "    - Uses hierarchical splits to classify text.\n",
    "    - Random Forests aggregate decisions from multiple trees for robustness.\n",
    "-----\n",
    "\n",
    "**Deep Learning Models**\n",
    "\n",
    "These models automatically learn feature representations from raw text or embeddings.\n",
    "\n",
    "- **Recurrent Neural Networks (RNNs):**\n",
    "    - Suited for sequential data, processes text one token at a time.\n",
    "    - Variants include LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit).\n",
    "- **Convolutional Neural Networks (CNNs):**\n",
    "    - Extract local patterns in text (e.g., n-grams).\n",
    "    - Often combined with embeddings for short text classification.\n",
    "- **Transformer-Based Models:**\n",
    "    - Utilize self-attention mechanisms to capture global and contextual relationships.\n",
    "    - Popular models include:\n",
    "        - BERT (Bidirectional Encoder Representations from Transformers): Pretrained and fine-tuned for specific tasks.\n",
    "        - GPT (Generative Pre-trained Transformer): Good for text generation and understanding.\n",
    "        - RoBERTa, DistilBERT, ALBERT: Variants of BERT optimized for specific tasks.\n",
    "- **Sequence-to-Sequence Models:**\n",
    "    - Useful for hierarchical classification or multi-label tasks.\n",
    "    - Encoder-decoder architectures (e.g., T5, BART).\n",
    "\n",
    "---- \n",
    "\n",
    "**Hybrid Models**\n",
    "\n",
    "- **ML + Deep Learning:**\n",
    "    - Combine traditional ML models with embeddings as features (e.g., train SVM using Word2Vec).\n",
    "- **Ensemble Methods:**\n",
    "    - Combine predictions from multiple models for improved performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Challenges in Text Classification\n",
    "- **Data Scarcity:** Insufficient labeled examples.\n",
    "- **Class Imbalance:** Uneven distribution of categories.\n",
    "- **Ambiguity:** Words with multiple meanings.\n",
    "- **Domain Adaptation:** Models trained on one domain may not generalize well to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps in details\n",
    "\n",
    "### Data Collection\n",
    "- **Objective:** Gather a dataset that contains text samples and their corresponding labels (categories).\n",
    "- **Sources of Data:**\n",
    "    - *Public Datasets:* IMDB reviews (sentiment analysis), 20 Newsgroups (news classification), SMS spam collection (spam detection).\n",
    "    - *Scraping/Web APIs:* Extract text from news articles, product reviews, or social media.\n",
    "    - *Internal Data:* Emails, customer feedback, chatbot logs, or support tickets.\n",
    "\n",
    "- **Example:**\n",
    "\n",
    "    üìå Source: Customer Support Dataset\n",
    "\n",
    "    üìå Description: 3 million tweets from customers interacting with company support teams.\n",
    "\n",
    "    üìå Data Sample:\n",
    "    Query| Category\n",
    "    ---|:---:\t\n",
    "    \"My internet is not working for the last 3 hours. Please help!\"\t| Technical Issue\n",
    "    \"How can I update my payment method?\" | Billing\n",
    "\n",
    "### **Text Preprocessing**\n",
    "\n",
    "Text preprocessing is a crucial step in Natural Language Processing (NLP) that ensures text data is clean and structured before being used in a model. This step involves transforming raw text into a format that can be effectively processed by machine learning algorithms.\n",
    "\n",
    "- **Tokenization**\n",
    "    - Definition:\n",
    "    Tokenization is the process of breaking a text string into smaller units (tokens), which can be words, subwords, or even characters.\n",
    "\n",
    "    - Types of Tokenization:\n",
    "\n",
    "        a) Word Tokenization\n",
    "        - Splits text into words based on spaces or punctuation.\n",
    "        - Example:\n",
    "            - Input: `\"Text classification is important!\"`\n",
    "            - Output: `[\"Text\", \"classification\", \"is\", \"important\", \"!\"]`\n",
    "        - Methods:\n",
    "            - Python Libraries: `nltk.word_tokenize()`, `spaCy.tokenizer`\n",
    "            - Regular Expressions: `re.split(\"\\s+\", text)`\n",
    "\n",
    "        b) Subword Tokenization\n",
    "\n",
    "        - Breaks words into smaller meaningful units, useful for handling unseen words.\n",
    "        - Example:\n",
    "            - `\"unhappiness\"` ‚Üí `[\"un\", \"happiness\"]`\n",
    "        - Methods:\n",
    "            - Byte Pair Encoding (BPE)\n",
    "            - Unigram Language Model\n",
    "            - WordPiece (used in BERT)\n",
    "\n",
    "        c) Character Tokenization\n",
    "\n",
    "        - Splits text into individual characters.\n",
    "        - Example: `\"hello\"` ‚Üí `[\"h\", \"e\", \"l\", \"l\", \"o\"]`\n",
    "        - Used in: Speech recognition, OCR tasks\n",
    "- **Lowercasing**\n",
    "    - Definition:\n",
    "    Converting all text to lowercase to ensure uniformity and avoid case sensitivity issues.\n",
    "\n",
    "    - Methods:\n",
    "        - Python: `text.lower()`\n",
    "        - NLTK: `text.casefold()`\n",
    "    - Why It‚Äôs Important:\n",
    "        - Reduces vocabulary size (\"Apple\" and \"apple\" are treated as the same word).\n",
    "        - Some models (like BERT) do not require this as they maintain case sensitivity.\n",
    "\n",
    "- **Stopword Removal**\n",
    "    - Definition:\n",
    "    Stopwords are common words like \"the\", \"is\", \"and\", \"to\", which appear frequently but do not contribute much meaning.\n",
    "\n",
    "    - Methods:\n",
    "\n",
    "        - NLTK:\n",
    "        ```python\n",
    "            from nltk.corpus import stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            words = [word for word in tokens if word not in stop_words]\n",
    "        ```\n",
    "        - spaCy:\n",
    "        ```python\n",
    "            import spacy\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "            tokens = [token.text for token in nlp(text) if not token.is_stop]\n",
    "        ```\n",
    "    - Considerations:\n",
    "        - Removing stopwords can sometimes remove useful words in sentiment analysis (e.g., ‚Äúnot‚Äù in \"I do not like it\").\n",
    "        - Some tasks may benefit from keeping stopwords.\n",
    "\n",
    "- **Stemming**\n",
    "    - Definition: Reducing a word to its root by chopping off prefixes or suffixes.\n",
    "    - Methods:\n",
    "        - Porter Stemmer (NLTK):\n",
    "        ```python\n",
    "            from nltk.stem import PorterStemmer\n",
    "            ps = PorterStemmer()\n",
    "            print(ps.stem(\"running\"))  # Output: run\n",
    "        ```\n",
    "        - Snowball Stemmer: More aggressive than Porter.\n",
    "        - Lancaster Stemmer: Even more aggressive.\n",
    "        \n",
    "    - Example:\n",
    "\n",
    "        Word | Stemmed Form (Porter)\n",
    "        ---|:---:|\n",
    "        Running\t| Run\n",
    "        Studies |\tStudi\n",
    "        Happily |\tHappili\n",
    "\n",
    "    - Pros: Computationally efficient.\n",
    "    - Cons: May not produce real words.\n",
    "\n",
    "- **Lemmatization**\n",
    "    - Definition: Reduces words to their base dictionary form using linguistic knowledge.\n",
    "    - Methods:\n",
    "        - NLTK WordNet Lemmatizer:\n",
    "        ```python\n",
    "            from nltk.stem import WordNetLemmatizer\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            print(lemmatizer.lemmatize(\"running\", pos=\"v\"))  # Output: run\n",
    "        ```\n",
    "        - spaCy Lammatizer:\n",
    "        ```python\n",
    "            import spacy\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "            doc = nlp(\"running jumped studies\")\n",
    "            print([token.lemma_ for token in doc])  # Output: ['run', 'jump', 'study']\n",
    "        ```\n",
    "    - Example:\n",
    "\n",
    "        Word | Lemmatized Form\n",
    "        ---|:---:|\n",
    "        Running\t| Run\n",
    "        Studies\t| Study\n",
    "        Mice | Mouse\n",
    "    - Pros: Produces real words.\n",
    "    - Cons: Slower than stemming\n",
    "- **Handling Special Characters & Punctuation**\n",
    "    - Definition:\n",
    "    Removing or replacing symbols, emojis, and punctuation to standardize text.\n",
    "\n",
    "    - Methods:\n",
    "        - Removing Punctuation:\n",
    "        ```python\n",
    "            import string\n",
    "            text = \"Hello!!! How are you?\"\n",
    "            text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "            print(text)  # Output: Hello How are you\n",
    "        ```\n",
    "        - Replacing Emojis (Emoji Dictionary Approach):\n",
    "        ```python\n",
    "            import emoji\n",
    "            print(emoji.demojize(\"I love Python! üòç\"))  # Output: I love Python! :heart_eyes:\n",
    "        ```\n",
    "        - Removing HTML Tags:\n",
    "        ```python\n",
    "            from bs4 import BeautifulSoup\n",
    "            clean_text = BeautifulSoup(\"<p>Hello</p>\", \"html.parser\").text\n",
    "        ```\n",
    "- Text Normalization\n",
    "    - Definition:\n",
    "    Converting different forms of words into a single standard form.\n",
    "\n",
    "    - Methods:\n",
    "        - Expanding contractions: \"you're\" ‚Üí \"you are\"\n",
    "        ```python\n",
    "            from contractions import fix\n",
    "            print(fix(\"You're going to school.\"))  # Output: You are going to school.\n",
    "        ```\n",
    "    - Converting numbers to words: \"100\" ‚Üí \"one hundred\"\n",
    "        ```python\n",
    "            import inflect\n",
    "            p = inflect.engine()\n",
    "            print(p.number_to_words(100))  # Output: one hundred\n",
    "        ```\n",
    "### Feature Extraction\n",
    "- Objective: Convert raw text into numerical representations that models can understand.\n",
    "\n",
    "    a) Bag of Words (BoW)\n",
    "    - Definition: Represents text as a count of words in a document.\n",
    "    - Example:\n",
    "        ```vbnet\n",
    "            Text 1: \"I love machine learning\"\n",
    "            Text 2: \"Machine learning is great\"\n",
    "        ```\n",
    "        BoW Representation:\n",
    "        ```arduino\n",
    "            \"I\" - 1, \"love\" - 1, \"machine\" - 1, \"learning\" - 2, \"is\" - 1, \"great\" - 1\n",
    "        ```\n",
    "    b) TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "    - Definition: Assigns importance to words by considering how often they appear in a document vs. across all documents.\n",
    "    - Formula:\n",
    "        $$\n",
    "        TF = \\frac{Number¬†of¬†times¬†a¬†word¬†appears¬†in¬†a¬†document}{Total¬†words¬†in¬†the¬†document}\\\\\\\\\n",
    "        $$\n",
    "        $$\n",
    "        IDF = log(\\frac{Total¬†documents}{Number¬†of¬†documents¬†containing¬†the¬†word}+1)\n",
    "        $$\n",
    "\n",
    "    c) Word Embeddings\n",
    "    - Definition: Transforms words into dense vector representations capturing their meaning.\n",
    "    - Techniques:\n",
    "        - Word2Vec (CBOW & Skip-gram)\n",
    "        - GloVe\n",
    "        - FastText\n",
    "        \n",
    "    d) Sentence Embeddings\n",
    "    - Definition: Captures meaning at the sentence level instead of word level.\n",
    "    - Example Models:\n",
    "        - BERT (Bidirectional Encoder Representations from Transformers)\n",
    "        - Sentence Transformers\n",
    "    - Example Use Case:\n",
    "        \"The bank is by the river\" vs. \"I deposited money in the bank\" (understanding context better).\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Language Models (SLMs)\n",
    "\n",
    "- Definition:\n",
    "    A language model assigns probabilities to sequences of words. It helps predict the next word in a sequence based on previous words.\n",
    "\n",
    "- Common Statistical Language Models:\n",
    "    - Unigram Model\n",
    "\n",
    "        - Assumes each word is independent of previous words.\n",
    "\n",
    "        - Probability of a sentence:\n",
    "            $$\n",
    "            P(w_1, w_2, ..., w_n) = \\Pi_i P(w_i)\n",
    "            $$\n",
    "            Example: \"I love NLP\"\n",
    "            $$\n",
    "            P(I)\\times P(love)\\times P(NLP)\n",
    "            $$\n",
    "\n",
    "        - Limitation: Ignores word dependencies.\n",
    "    - Bigram Model\n",
    "        - Considers dependencies between adjacent words.\n",
    "        - Probability:\n",
    "            $$\n",
    "            P(w_1, w_2, ..., w_n) = \\Pi_i P(w_i|w_{i-1})\n",
    "            $$\n",
    "            Example: \"I love NLP\"\n",
    "            $$\n",
    "            P(I)\\times P(love|I)\\times P(NLP|love)\n",
    "            $$\n",
    "\n",
    "        - Captures local word relationships.\n",
    "    - Higher-order N-gram Models (Trigram, 4-gram, etc.)\n",
    "        - Extends to longer sequences:\n",
    "            $$\n",
    "            P(w_1, w_2, ..., w_n) = \\Pi_i P(w_i|w_{i-1}, w_{i-2}, ..., w_k), \\forall 1\\leq k \\leq i\n",
    "            $$\n",
    "        - More context but requires more data.\n",
    "- Smoothing Techniques:\n",
    "\n",
    "    To handle unseen words in training data:\n",
    "\n",
    "    - Laplace Smoothing: Adds a small constant to all word probabilities.\n",
    "    - Backoff & Interpolation: Uses lower-order models when higher-order models fail."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
