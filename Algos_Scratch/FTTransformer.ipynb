{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f40598",
   "metadata": {},
   "source": [
    "\n",
    "## FT-Transformer (Feature Tokenizer + Transformer)\n",
    "```plaintext[]\n",
    "Input Features  \n",
    "│  \n",
    "├─ Feature Tokenizer (Embedding Layer)  \n",
    "│  │  \n",
    "│  └─ Continuous Features → Linear Projection  \n",
    "│  └─ Categorical Features → Embedding Lookup  \n",
    "│  \n",
    "└─ Transformer Encoder (Multiple Layers)  \n",
    "   │  \n",
    "   └─ Multi-Head Self-Attention  \n",
    "   └─ Layer Normalization & Feed-Forward  \n",
    "   └─ Residual Connections  \n",
    "│  \n",
    "└─ Prediction Head (MLP for Regression/Classification)  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb3717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7151220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTokenizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Tokenizes both numerical and categorical features.\n",
    "    Numerical features are projected to d_model dimension.\n",
    "    Categorical features are embedded.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_numerical_features, cat_cardinalities, d_model):\n",
    "        super().__init__()\n",
    "        self.num_numerical = num_numerical_features\n",
    "        self.cat_cardinalities = cat_cardinalities\n",
    "        \n",
    "        if num_numerical_features > 0:\n",
    "            self.num_projection = nn.Linear(num_numerical_features, d_model)\n",
    "        \n",
    "        if cat_cardinalities:\n",
    "            self.cat_embeddings = nn.ModuleList([\n",
    "                nn.Embedding(cardinality, d_model) \n",
    "                for cardinality in cat_cardinalities\n",
    "            ])\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x_num, x_cat=None):\n",
    "        tokens = []\n",
    "        \n",
    "        if self.num_numerical > 0:\n",
    "            num_tokens = self.num_projection(x_num).unsqueeze(1)\n",
    "            tokens.append(num_tokens)\n",
    "        \n",
    "        if x_cat is not None and self.cat_cardinalities:\n",
    "            cat_tokens = []\n",
    "            for i, embed in enumerate(self.cat_embeddings):\n",
    "                cat_tokens.append(embed(x_cat[:, i]))\n",
    "            cat_tokens = torch.stack(cat_tokens, dim=1)\n",
    "            tokens.append(cat_tokens)\n",
    "        \n",
    "        tokens = torch.cat(tokens, dim=1)\n",
    "        cls_tokens = self.cls_token.expand(x_num.size(0), -1, -1)\n",
    "        tokens = torch.cat([cls_tokens, tokens], dim=1)\n",
    "        return self.norm(tokens)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard positional encoding for transformers (Optional)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af454cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    FT-Transformer for tabular data with mixed numerical and categorical features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, preprocessor, d_model=64, nhead=4, num_layers=3,\n",
    "                 dim_feedforward=128, dropout=0.1, num_classes=None,\n",
    "                 use_pos_encoding=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            preprocessor: Fitted DataFramePreprocessor instance\n",
    "            d_model: Transformer model dimension\n",
    "            nhead: Number of attention heads\n",
    "            num_layers: Number of TransformerEncoder layers\n",
    "            dim_feedforward: Dimension of feedforward network\n",
    "            dropout: Dropout rate\n",
    "            num_classes: Number of output classes (None for regression)\n",
    "            use_pos_encoding: Whether to use positional encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.preprocessor = preprocessor\n",
    "        self.use_pos_encoding = use_pos_encoding\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Feature tokenizer\n",
    "        self.feature_tokenizer = FeatureTokenizer(\n",
    "            num_numerical_features=len(preprocessor.num_cols),\n",
    "            cat_cardinalities=preprocessor.cat_cardinalities,\n",
    "            d_model=d_model\n",
    "        )\n",
    "\n",
    "        # Calculate total number of tokens\n",
    "        num_tokens = 1  # CLS token\n",
    "        if len(preprocessor.num_cols) > 0:\n",
    "            num_tokens += 1  # One token for all numerical features\n",
    "        if preprocessor.cat_cardinalities:\n",
    "            num_tokens += len(preprocessor.cat_cardinalities)\n",
    "\n",
    "        # Optional positional encoding\n",
    "        if self.use_pos_encoding:\n",
    "            self.pos_encoder = PositionalEncoding(d_model, max_len=num_tokens)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output head\n",
    "        if num_classes is not None:\n",
    "            self.head = nn.Linear(d_model, num_classes)  # Binary or multi-class classification\n",
    "        else:\n",
    "            self.head = nn.Linear(d_model, 1)  # Regression\n",
    "\n",
    "    def forward_from_processed(self, x_num, x_cat):\n",
    "        \"\"\"\n",
    "        Forward pass with already processed numerical and categorical tensors.\n",
    "        \"\"\"\n",
    "        tokens = self.feature_tokenizer(x_num, x_cat)\n",
    "\n",
    "        if self.use_pos_encoding:\n",
    "            tokens = self.pos_encoder(tokens)\n",
    "\n",
    "        encoded = self.transformer_encoder(tokens)\n",
    "        cls_output = encoded[:, 0, :]  # CLS token\n",
    "\n",
    "        output = self.head(cls_output)\n",
    "\n",
    "        # Return raw logits or regression output\n",
    "        return output\n",
    "\n",
    "    def forward(self, df):\n",
    "        \"\"\"\n",
    "        Forward pass directly from a raw DataFrame.\n",
    "        \"\"\"\n",
    "        x_num, x_cat = self.preprocessor.transform(df)\n",
    "        return self.forward_from_processed(x_num, x_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3efc520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFramePreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocesses a pandas DataFrame for the FT-Transformer\n",
    "    - Identifies numerical and categorical columns\n",
    "    - Normalizes numerical features\n",
    "    - Encodes categorical features\n",
    "    - Handles missing values\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.num_cols = []\n",
    "        self.cat_cols = []\n",
    "        self.cat_cardinalities = []\n",
    "        self.num_scaler = StandardScaler()\n",
    "        self.cat_encoders = defaultdict(LabelEncoder)\n",
    "        self.fitted = False\n",
    "        \n",
    "    def fit(self, df):\n",
    "        \"\"\"Identify and prepare feature processors\"\"\"\n",
    "        # Identify feature types\n",
    "        self.num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "        self.cat_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n",
    "        \n",
    "        # Fit numerical scaler\n",
    "        if self.num_cols:\n",
    "            self.num_scaler.fit(df[self.num_cols].fillna(0).values)\n",
    "        \n",
    "        # Fit categorical encoders and get cardinalities\n",
    "        self.cat_cardinalities = []\n",
    "        for col in self.cat_cols:\n",
    "            # Fill NA with a special category\n",
    "            series = df[col].fillna('__NA__').astype(str)\n",
    "            self.cat_encoders[col].fit(series)\n",
    "            self.cat_cardinalities.append(len(self.cat_encoders[col].classes_))\n",
    "        \n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"Transform a DataFrame into processed numerical and categorical tensors\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Preprocessor must be fit before transforming data\")\n",
    "        \n",
    "        # Process numerical features\n",
    "        x_num = torch.empty(0)\n",
    "        if self.num_cols:\n",
    "            num_values = self.num_scaler.transform(df[self.num_cols].fillna(0).values)\n",
    "            x_num = torch.FloatTensor(num_values)\n",
    "        \n",
    "        # Process categorical features\n",
    "        x_cat = torch.empty(0)\n",
    "        if self.cat_cols:\n",
    "            cat_data = []\n",
    "            for col in self.cat_cols:\n",
    "                series = df[col].fillna('__NA__').astype(str)\n",
    "                encoded = self.cat_encoders[col].transform(series)\n",
    "                cat_data.append(encoded)\n",
    "            x_cat = torch.LongTensor(np.column_stack(cat_data))\n",
    "        \n",
    "        return x_num, x_cat\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408c1dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    \"\"\"Dataset for handling pandas DataFrames\"\"\"\n",
    "    def __init__(self, df, preprocessor, is_classification=True):\n",
    "        self.x_num, self.x_cat = preprocessor.transform(df)\n",
    "        self.is_classification = is_classification\n",
    "        # Convert to long for classification, float for regression\n",
    "        if is_classification:\n",
    "            self.y = torch.LongTensor(df['target'].values)  # Changed to LongTensor\n",
    "        else:\n",
    "            self.y = torch.FloatTensor(df['target'].values)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.x_cat.nelement() == 0:  # No categorical features\n",
    "            return self.x_num[idx], self.y[idx]\n",
    "        return self.x_num[idx], self.x_cat[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39679e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10, device='cpu'):\n",
    "    \"\"\"Training loop with validation\"\"\"\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            if len(batch) == 2:  # Only numerical features\n",
    "                x_num, y = batch\n",
    "                x_cat = None\n",
    "            else:  # Both numerical and categorical\n",
    "                x_num, x_cat, y = batch\n",
    "            \n",
    "            x_num = x_num.to(device)\n",
    "            y = y.to(device if model.num_classes is None else torch.long)  # Ensure correct type\n",
    "            if x_cat is not None:\n",
    "                x_cat = x_cat.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model.forward_from_processed(x_num, x_cat)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * x_num.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                if len(batch) == 2:\n",
    "                    x_num, y = batch\n",
    "                    x_cat = None\n",
    "                else:\n",
    "                    x_num, x_cat, y = batch\n",
    "                \n",
    "                x_num, y = x_num.to(device), y.to(device)\n",
    "                if x_cat is not None:\n",
    "                    x_cat = x_cat.to(device)\n",
    "                \n",
    "                outputs = model.forward_from_processed(x_num, x_cat)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item() * x_num.size(0)\n",
    "                \n",
    "                all_preds.append(outputs.cpu())\n",
    "                all_targets.append(y.cpu())\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if model.num_classes is not None:  # Classification\n",
    "            preds = torch.argmax(all_preds, dim=1)\n",
    "            targets = all_targets.long()\n",
    "            accuracy = accuracy_score(targets, preds)\n",
    "            print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {accuracy:.4f}\")\n",
    "        else:  # Regression\n",
    "            rmse = mean_squared_error(all_targets, all_preds, squared=False)\n",
    "            print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    # model.load_state_dict(torch.load('best_model.pth'))  # Load best model\n",
    "    return model\n",
    "\n",
    "def predictions(model, df):\n",
    "    with torch.no_grad():\n",
    "        predictions_prob = model(df)\n",
    "        predictions = np.argmax(predictions_prob, axis=1)\n",
    "    df['prediction'] = predictions\n",
    "    return df\n",
    "\n",
    "def confusion_matrix(prediction_df):\n",
    "    y = prediction_df['target'].values\n",
    "    y_pred = prediction_df['prediction'].values\n",
    "    TP = FP = TN = FN = 0\n",
    "    for t, p in zip(y, y_pred):\n",
    "        TP += t == 1 and p == 1\n",
    "        FP += t == 0 and p == 1\n",
    "        TN += t == 0 and p == 0\n",
    "        FN += t == 1 and p == 0\n",
    "    return np.array([[TP, FP], [FN, TN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe72aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target \n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e793c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_df, val_df = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess data\n",
    "preprocessor = DataFramePreprocessor().fit(train_df)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TabularDataset(train_df, preprocessor)\n",
    "val_dataset = TabularDataset(val_df, preprocessor)\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd761e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training: 100%|██████████| 24/24 [00:00<00:00, 106.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.5301, Val Loss: 0.1237, Val Acc: 0.9583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training: 100%|██████████| 24/24 [00:00<00:00, 109.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0466, Val Loss: 0.0889, Val Acc: 0.9583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training: 100%|██████████| 24/24 [00:00<00:00, 107.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0189, Val Loss: 0.0330, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Training: 100%|██████████| 24/24 [00:00<00:00, 104.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0105, Val Loss: 0.0127, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Training: 100%|██████████| 24/24 [00:00<00:00, 108.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.0080, Val Loss: 0.0112, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Training: 100%|██████████| 24/24 [00:00<00:00, 100.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.0071, Val Loss: 0.0056, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Training: 100%|██████████| 24/24 [00:00<00:00, 75.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.0057, Val Loss: 0.0049, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Training: 100%|██████████| 24/24 [00:00<00:00, 105.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.0048, Val Loss: 0.0045, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Training: 100%|██████████| 24/24 [00:00<00:00, 107.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.0044, Val Loss: 0.0041, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Training: 100%|██████████| 24/24 [00:00<00:00, 105.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.0037, Val Loss: 0.0036, Val Acc: 1.0000\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = FTTransformer(\n",
    "    preprocessor=preprocessor,\n",
    "    d_model=64,\n",
    "    nhead=4,\n",
    "    num_layers=3,\n",
    "    dim_feedforward=128,\n",
    "    dropout=0.1,\n",
    "    num_classes=4  # Binary classification\n",
    ")\n",
    "\n",
    "# Set up training\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "# criterion = nn.MSELoss()  # For regression\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    epochs=epochs,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df680b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  0],\n",
       "       [ 0, 10]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predictions(trained_model, df_test)\n",
    "confusion_matrix(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc3908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
